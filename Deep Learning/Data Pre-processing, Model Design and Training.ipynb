{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_heZlLz5VucO"
   },
   "source": [
    "**Note: Data pre-processing**  \n",
    "Procedures:  \n",
    "&nbsp; 1): Read the dataset  \n",
    "&nbsp; 2): Transform the tabular data into images  \n",
    "&nbsp; 3): Display the transformed images  \n",
    "&nbsp; 4): Split the training and test set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-H7ONwMVucR",
    "outputId": "c0475789-232c-4e7c-b977-45ea6cad2d38"
   },
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeS2rkdMVucS"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "76xOtoViVucT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Reshape\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d-or3dRDVucU"
   },
   "outputs": [],
   "source": [
    "#Read dataset\n",
    "df=pd.read_csv('10_percentage_of_each_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "8-HabfGhVucU",
    "outputId": "89b281cb-0a24-4ddb-d753-616b03c44039",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# subset = df.sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('CAN ID', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2rJ4VksVucV",
    "outputId": "284654b2-b0ed-44ec-8293-6215b8591e27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R        1403673\n",
       "RPM        65439\n",
       "gear       60016\n",
       "Dos        58469\n",
       "Fuzzy      49258\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The labels of the dataset. \"R\" indicates normal patterns, and there are four types of attack (DoS, fuzzy. gear spoofing, and RPM spoofing zttacks)\n",
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Spya_5QqVucV"
   },
   "source": [
    "## Data Transformation\n",
    "Convert tabular data to images\n",
    "Procedures:\n",
    "1. Use quantile transform to transform the original data samples into the scale of [0,255], representing pixel values\n",
    "2. Generate images for each category (Normal, DoS, Fuzzy, Gear, RPM), each image consists of 27 data samples with 9 features. Thus, the size of each image is 9*9*3, length 9, width 9, and 3 color channels (RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "J_sHg8-dVucW"
   },
   "outputs": [],
   "source": [
    "# Transform all features into the scale of [0,1]\n",
    "numeric_features = df.dtypes[df.dtypes != 'object'].index\n",
    "scaler = QuantileTransformer() \n",
    "df[numeric_features] = scaler.fit_transform(df[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Kv1NmiKaVucW"
   },
   "outputs": [],
   "source": [
    "# Multiply the feature values by 255 to transform them into the scale of [0,255]\n",
    "df[numeric_features] = df[numeric_features].apply(\n",
    "    lambda x: (x*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNEsv7yvVucW"
   },
   "outputs": [],
   "source": [
    "# df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "oODqvjEXVucW"
   },
   "source": [
    "All features are in the same scale of [0,255]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6p8x6G8VucX"
   },
   "source": [
    "### Generate images for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ibx9JRVUVucX"
   },
   "outputs": [],
   "source": [
    "# df0=df[df['Label']=='R'].drop(['Label'],axis=1)\n",
    "df1=df[df['Label']=='RPM'].drop(['Label'],axis=1)\n",
    "df2=df[df['Label']=='gear'].drop(['Label'],axis=1)\n",
    "df3=df[df['Label']=='Dos'].drop(['Label'],axis=1)\n",
    "df4=df[df['Label']=='Fuzzy'].drop(['Label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "2WP1A3WDVucX",
    "outputId": "ead0f0e8-94d4-46ae-d54a-4fcdb050331b"
   },
   "outputs": [],
   "source": [
    "# # Generate 9*9 color images for class 0 (Normal)\n",
    "# # Change the numbers 9 to the number of features n in your dataset if you use a different dataset, reshape(n,n,3)\n",
    "# count=0\n",
    "# ims = []\n",
    "\n",
    "# image_path = \"train/0/\"\n",
    "# os.makedirs(image_path)\n",
    "\n",
    "# for i in range(0, len(df0)):  \n",
    "#     count=count+1\n",
    "#     if count<=27: \n",
    "#         im=df0.iloc[i].values\n",
    "#         ims=np.append(ims,im)\n",
    "#     else:\n",
    "#         ims=np.array(ims).reshape(9,9,3)\n",
    "#         array = np.array(ims, dtype=np.uint8)\n",
    "#         new_image = Image.fromarray(array)\n",
    "#         new_image.save(image_path+str(i)+'.png')\n",
    "#         count=0\n",
    "#         ims = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Z6yzxnCRVucX"
   },
   "outputs": [],
   "source": [
    "# Generate 9*9 color images for class 1 (RPM spoofing)\n",
    "count=0\n",
    "ims = []\n",
    "\n",
    "image_path = \"train/0/\"\n",
    "os.makedirs(image_path)\n",
    "\n",
    "for i in range(0, len(df1)):  \n",
    "    count=count+1\n",
    "    if count<=24: \n",
    "        im=df1.iloc[i].values\n",
    "        ims=np.append(ims,im)\n",
    "    else:\n",
    "        ims=np.array(ims).reshape(8,8,3)\n",
    "        array = np.array(ims, dtype=np.uint8)\n",
    "        new_image = Image.fromarray(array)\n",
    "        new_image.save(image_path+str(i)+'.png')\n",
    "        count=0\n",
    "        ims = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rgjgMSQuVucY"
   },
   "outputs": [],
   "source": [
    "# Generate 9*9 color images for class 2 (Gear spoofing)\n",
    "count=0\n",
    "ims = []\n",
    "\n",
    "image_path = \"train/1/\"\n",
    "os.makedirs(image_path)\n",
    "\n",
    "for i in range(0, len(df2)):  \n",
    "    count=count+1\n",
    "    if count<=24: \n",
    "        im=df2.iloc[i].values\n",
    "        ims=np.append(ims,im)\n",
    "    else:\n",
    "        ims=np.array(ims).reshape(8,8,3)\n",
    "        array = np.array(ims, dtype=np.uint8)\n",
    "        new_image = Image.fromarray(array)\n",
    "        new_image.save(image_path+str(i)+'.png')\n",
    "        count=0\n",
    "        ims = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FaXhdUThVucY"
   },
   "outputs": [],
   "source": [
    "# Generate 9*9 color images for class 3 (DoS attack)\n",
    "count=0\n",
    "ims = []\n",
    "\n",
    "image_path = \"train/2/\"\n",
    "os.makedirs(image_path)\n",
    "\n",
    "\n",
    "for i in range(0, len(df3)):  \n",
    "    count=count+1\n",
    "    if count<=24: \n",
    "        im=df3.iloc[i].values\n",
    "        ims=np.append(ims,im)\n",
    "    else:\n",
    "        ims=np.array(ims).reshape(8,8,3)\n",
    "        array = np.array(ims, dtype=np.uint8)\n",
    "        new_image = Image.fromarray(array)\n",
    "        new_image.save(image_path+str(i)+'.png')\n",
    "        count=0\n",
    "        ims = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OTTJNLWSVucY"
   },
   "outputs": [],
   "source": [
    "# Generate 9*9 color images for class 4 (Fuzzy attack)\n",
    "count=0\n",
    "ims = []\n",
    "\n",
    "image_path = \"train/3/\"\n",
    "os.makedirs(image_path)\n",
    "\n",
    "\n",
    "for i in range(0, len(df4)):  \n",
    "    count=count+1\n",
    "    if count<=24: \n",
    "        im=df4.iloc[i].values\n",
    "        ims=np.append(ims,im)\n",
    "    else:\n",
    "        ims=np.array(ims).reshape(8,8,3)\n",
    "        array = np.array(ims, dtype=np.uint8)\n",
    "        new_image = Image.fromarray(array)\n",
    "        new_image.save(image_path+str(i)+'.png')\n",
    "        count=0\n",
    "        ims = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13I8B7PzVucY"
   },
   "source": [
    "### Display samples for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IGmQGvMyVucZ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read the images for each category, the file name may vary (27.png, 83.png...)\n",
    "img1 = Image.open('./train/0/24.png')\n",
    "img2 = Image.open('./train/1/24.png')\n",
    "img3 = Image.open('./train/2/24.png')\n",
    "img4 = Image.open('./train/3/24.png')\n",
    "# img5 = Image.open('./train/4/27.png')\n",
    "\n",
    "plt.figure(figsize=(10, 10)) \n",
    "plt.subplot(1,5,1)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"RPM Spoofing\")\n",
    "plt.subplot(1,5,2)\n",
    "plt.imshow(img2)\n",
    "plt.title(\"Gear Spoofing\")\n",
    "plt.subplot(1,5,3)\n",
    "plt.imshow(img3)\n",
    "plt.title(\"DoS Attack\")\n",
    "plt.subplot(1,5,4)\n",
    "plt.imshow(img4)\n",
    "plt.title(\"Fuzzy Attack\")\n",
    "# plt.subplot(1,5,5)\n",
    "# plt.imshow(img5)\n",
    "# plt.title(\"Fuzzy Attack\")\n",
    "plt.show()  # display it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXAfI4FFVucZ"
   },
   "source": [
    "## Split the training and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oiIIn1ZOVucZ",
    "outputId": "f6ff6889-c645-49f8-ccbf-2142f69a99ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9325\n"
     ]
    }
   ],
   "source": [
    "# Create folders to store images\n",
    "Train_Dir='./train/'\n",
    "Val_Dir='./test/'\n",
    "allimgs=[]\n",
    "for subdir in os.listdir(Train_Dir):\n",
    "    for filename in os.listdir(os.path.join(Train_Dir,subdir)):\n",
    "        filepath=os.path.join(Train_Dir,subdir,filename)\n",
    "        allimgs.append(filepath)\n",
    "print(len(allimgs)) # Print the total number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hQRNhRXUVucZ"
   },
   "outputs": [],
   "source": [
    "#split a test set from the dataset, train/test size = 80%/20%\n",
    "Numbers=len(allimgs)//5 \t#size of test set (20%)\n",
    "\n",
    "def mymovefile(srcfile,dstfile):\n",
    "    if not os.path.isfile(srcfile):\n",
    "#         print (\"%s not exist!\"%(srcfile))\n",
    "        pass\n",
    "    else:\n",
    "        fpath,fname=os.path.split(dstfile)    \n",
    "        if not os.path.exists(fpath):\n",
    "            os.makedirs(fpath)               \n",
    "        shutil.move(srcfile,dstfile)          \n",
    "        #print (\"move %s -> %s\"%(srcfile,dstfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Ej5Uk5IVuca",
    "outputId": "c4a5c99d-8f12-4bbe-d6dc-38e678310add",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1865"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The size of test set\n",
    "Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zw2vQ_dYVuca",
    "outputId": "3968b5b8-01a3-4460-aa48-800b8798d6d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish creating test set\n"
     ]
    }
   ],
   "source": [
    "# Create the test set\n",
    "val_imgs=random.sample(allimgs,Numbers)\n",
    "for img in val_imgs:\n",
    "    dest_path=img.replace(Train_Dir,Val_Dir)\n",
    "    mymovefile(img,dest_path)\n",
    "print('Finish creating test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "C5GGJtcGVuca"
   },
   "outputs": [],
   "source": [
    "#resize the images 224*224 for better CNN training\n",
    "def get_224(folder,dstdir):\n",
    "    imgfilepaths=[]\n",
    "    for root,dirs,imgs in os.walk(folder):\n",
    "        for thisimg in imgs:\n",
    "            thisimg_path=os.path.join(root,thisimg)\n",
    "            imgfilepaths.append(thisimg_path)\n",
    "    for thisimg_path in imgfilepaths:\n",
    "        dir_name,filename=os.path.split(thisimg_path)\n",
    "        dir_name=dir_name.replace(folder,dstdir)\n",
    "        new_file_path=os.path.join(dir_name,filename)\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.makedirs(dir_name)\n",
    "        img=cv2.imread(thisimg_path)\n",
    "        img=cv2.resize(img,(224,224))\n",
    "        cv2.imwrite(new_file_path,img)\n",
    "    print('Finish resizing'.format(folder=folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkwFfjpyVuca",
    "outputId": "0b72712c-4ee6-4919-ae7e-f5e3c405a349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish resizing\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR_224='./train_224/'\n",
    "get_224(folder='./train/',dstdir=DATA_DIR_224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yzm5nTGDVuca",
    "outputId": "9a5bd276-69c4-4a19-ef09-bffb43105522"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish resizing\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR2_224='./test_224/'\n",
    "get_224(folder='./test/',dstdir=DATA_DIR2_224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRRWTlfEVuca"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dLIQTtA8WbX4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.preprocessing.image import  ImageDataGenerator\n",
    "from keras.layers import Dense,Flatten,GlobalAveragePooling2D,Input,Conv2D,MaxPooling2D,Dropout\n",
    "from keras.models import Model,load_model,Sequential\n",
    "# from keras.applications.xception import  Xception\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# from keras.applications.vgg19 import VGG19\n",
    "# from keras.applications.resnet50 import  ResNet50\n",
    "# from keras.applications.inception_v3 import InceptionV3\n",
    "# from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "# from keras.applications.mobilenet import MobileNet\n",
    "import keras.callbacks as kcallbacks\n",
    "import keras\n",
    "# from keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V4S4cleMWl1X",
    "outputId": "37974353-0b48-4c91-e822-8e6c10006cc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7463 images belonging to 4 classes.\n",
      "Found 1868 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "#generate training and test images\n",
    "TARGET_SIZE=(224,224)\n",
    "INPUT_SIZE=(224,224,3)\n",
    "BATCHSIZE=128\t#could try 128 or 32\n",
    "\n",
    "#Normalization\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './train_224/',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCHSIZE,\n",
    "        class_mode='categorical')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        './test_224/',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCHSIZE,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "pcuD4MFpXPEB"
   },
   "outputs": [],
   "source": [
    "#plot the figures\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # acc\n",
    "            plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "            # loss\n",
    "            plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "iVVs7LMTXVXY"
   },
   "outputs": [],
   "source": [
    "history_this= LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoRa73hcXXow"
   },
   "outputs": [],
   "source": [
    "def cnn_by_own(input_shape,num_class,epochs,savepath='./model_own.h5',history=history_this):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=input_shape,padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='glorot_uniform'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(num_class,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    #train model\n",
    "    earlyStopping=kcallbacks.EarlyStopping(monitor='val_loss', patience=0, verbose=1, mode='auto')\n",
    "    # saveBestModel = kcallbacks.ModelCheckpoint(filepath=savepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./model/cnn_model_own.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "    hist=model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        callbacks=[earlyStopping,model_checkpoint_callback,history],\n",
    "    )\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 665
    },
    "id": "N1xdbLmnXiko",
    "outputId": "5aaf497a-f261-418f-d1dc-35122e6d4c1b"
   },
   "outputs": [],
   "source": [
    "# cnn_scratch_hist = cnn_by_own(input_shape=INPUT_SIZE,num_class=4,epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation accuracy - loss\n",
    "# plt.figure(figsize=(10,7))\n",
    "# plt.plot(cnn_scratch_hist.history['accuracy'])\n",
    "# plt.plot(cnn_scratch_hist.history['val_accuracy'])\n",
    "# plt.plot(cnn_scratch_hist.history['loss'])\n",
    "# plt.plot(cnn_scratch_hist.history['val_loss'])\n",
    "# plt.title('CNN Scratch Model accuracy and Loss')\n",
    "# plt.ylabel('Accuracy - Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['accuracy', 'val_accuracy','loss', 'val_loss'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_scratch_hist.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16( num_class, epochs,history=history_this,input_shape=INPUT_SIZE):\n",
    "    model_fine_tune = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:15]: #the number of frozen layers for transfer learning, have tuned from 5-18\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[15:]:\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output) #GlobalAveragePooling2D layer to convert the features to a single 1280-element vector per image\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='vgg')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)  #tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping( monitor='val_loss', patience=3, verbose=1, mode='auto') #set early stop patience to save training time\n",
    "    saveBestModel = tf.keras.callbacks.ModelCheckpoint(filepath='./model',save_weights_only=True,monitor='val_accuracy',mode='max',save_best_only=True)\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        #workers=2,\n",
    "        callbacks=[earlyStopping, saveBestModel, history],\n",
    "    )\n",
    "    model.save('vgg_model.h5')\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 22:26:27.005592: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1644167168 exceeds 10% of free system memory.\n",
      "2023-03-28 22:26:28.677452: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1644167168 exceeds 10% of free system memory.\n",
      "2023-03-28 22:26:37.244812: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 822083584 exceeds 10% of free system memory.\n",
      "2023-03-28 22:26:40.760330: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 822083584 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 1/59 [..............................] - ETA: 1:07:31 - loss: 1.4466 - accuracy: 0.2656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 22:27:33.494091: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1644167168 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 2627s 44s/step - loss: 0.1389 - accuracy: 0.9496 - val_loss: 0.0470 - val_accuracy: 0.9984\n",
      "Epoch 2/5\n",
      "59/59 [==============================] - 2488s 42s/step - loss: 0.0126 - accuracy: 0.9995 - val_loss: 0.0475 - val_accuracy: 0.9984\n",
      "Epoch 3/5\n",
      "13/59 [=====>........................] - ETA: 25:12 - loss: 0.0122 - accuracy: 0.9981"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hist \u001b[38;5;241m=\u001b[39m\u001b[43mvgg16\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mvgg16\u001b[0;34m(num_class, epochs, history, input_shape)\u001b[0m\n\u001b[1;32m     15\u001b[0m earlyStopping \u001b[38;5;241m=\u001b[39m kcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping( monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#set early stop patience to save training time\u001b[39;00m\n\u001b[1;32m     16\u001b[0m saveBestModel \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model\u001b[39m\u001b[38;5;124m'\u001b[39m,save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m,save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 17\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#use_multiprocessing=True, \u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#workers=2,\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearlyStopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaveBestModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvgg_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hist\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/training.py:2604\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2592\u001b[0m \u001b[38;5;124;03m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[1;32m   2593\u001b[0m \n\u001b[1;32m   2594\u001b[0m \u001b[38;5;124;03mDEPRECATED:\u001b[39;00m\n\u001b[1;32m   2595\u001b[0m \u001b[38;5;124;03m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[1;32m   2596\u001b[0m \u001b[38;5;124;03m  use this endpoint.\u001b[39;00m\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2598\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2599\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.fit_generator` is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2600\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2601\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2602\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   2603\u001b[0m )\n\u001b[0;32m-> 2604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2616\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2618\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2619\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist =vgg16(num_class=4,epochs=5) #tf36cnn\n",
    "# history_this.loss_plot('epoch')\n",
    "# history_this.loss_plot('batch')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model = load_model('vgg_model.h5')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_accuracy = hist.history['accuracy'][-1]\n",
    "hist_loss = hist.history['loss'][-1]\n",
    "hist_val_accuracy=  hist.history['val_accuracy'][-1]\n",
    "hist_val_loss = hist.history['val_loss'][-1]\n",
    "print(\"Accucacy:\",round((hist_accuracy * 100),2),\"%\")\n",
    "print(\"Loss:\",round((hist_loss * 100),2),\"%\")\n",
    "print(\"Val_Accucacy:\",round((hist_val_accuracy * 100),2),\"%\")\n",
    "print(\"Val_Loss:\",round((hist_val_loss * 100),2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation accuracy - loss\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('CNN Model accuracy and Loss')\n",
    "plt.ylabel('Accuracy - Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['accuracy', 'val_accuracy','loss', 'val_loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy_cnn = hist.history['accuracy']\n",
    "validation_accuracy_cnn = hist.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy of the CNN model:\", round(training_accuracy_cnn[-1]*100,2), \"%\")\n",
    "print(\"Validation accuracy of the CNN model:\", round(validation_accuracy_cnn[-1]*100,2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lstm_model( num_class, epochs,history=history_this,input_shape=INPUT_SIZE):\n",
    "#     # Create the LSTM model\n",
    "#     model = Sequential()\n",
    "# #     model.add(Reshape((224, 224 *3), input_shape=(224, 224, 3)))\n",
    "#     model.add(Embedding(max_features, 32))\n",
    "#     model.add(LSTM(32))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "#     earlyStopping=kcallbacks.EarlyStopping(monitor='val_loss', patience=0, verbose=1, mode='auto')\n",
    "#         # saveBestModel = kcallbacks.ModelCheckpoint(filepath=savepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "#     model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"./model\",\n",
    "#         save_weights_only=True,\n",
    "#         monitor='val_accuracy',\n",
    "#         mode='max',\n",
    "#         save_best_only=True)\n",
    "\n",
    "#     # Train the model\n",
    "#     history=model.fit_generator(\n",
    "#         train_generator,\n",
    "#         steps_per_epoch=len(train_generator),\n",
    "#         epochs=epochs,\n",
    "#         validation_data=validation_generator,\n",
    "#         validation_steps=len(validation_generator),\n",
    "#         callbacks=[earlyStopping,model_checkpoint_callback,history_this]\n",
    "#         )\n",
    "#     model.save(\"lstm_model.h5\")\n",
    "#     return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_hist = lstm_model(num_class=4,epochs=10) #tf36cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model( num_class, epochs,history=history_this,input_shape=INPUT_SIZE):\n",
    "    # Create the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((224, 224 *3), input_shape=(224, 224, 3)))\n",
    "    model.add(LSTM(128))\n",
    "#     model.add(LSTM(64))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_class, activation='softmax'))\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)  #tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    earlyStopping=kcallbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "        # saveBestModel = kcallbacks.ModelCheckpoint(filepath=savepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"./model\",\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True)\n",
    "\n",
    "    # Train the model\n",
    "    history=model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        callbacks=[earlyStopping,model_checkpoint_callback,history_this]\n",
    "        )\n",
    "    model.save(\"lstm_model.h5\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 22:14:05.686096: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-03-28 22:14:05.686145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (maddy-Lenovo-ideapad-330-15IKB): /proc/driver/nvidia/version does not exist\n",
      "2023-03-28 22:14:05.687035: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "59/59 [==============================] - 71s 1s/step - loss: 0.1998 - accuracy: 0.9715 - val_loss: 0.0146 - val_accuracy: 0.9984\n",
      "Epoch 2/10\n",
      "59/59 [==============================] - 59s 996ms/step - loss: 0.0051 - accuracy: 0.9996 - val_loss: 0.0146 - val_accuracy: 0.9984\n",
      "Epoch 3/10\n",
      "59/59 [==============================] - 59s 988ms/step - loss: 0.0043 - accuracy: 0.9996 - val_loss: 0.0147 - val_accuracy: 0.9984\n",
      "Epoch 4/10\n",
      "59/59 [==============================] - 59s 998ms/step - loss: 0.0042 - accuracy: 0.9996 - val_loss: 0.0152 - val_accuracy: 0.9984\n",
      "Epoch 5/10\n",
      "59/59 [==============================] - 60s 1s/step - loss: 0.0041 - accuracy: 0.9996 - val_loss: 0.0155 - val_accuracy: 0.9984\n",
      "Epoch 6/10\n",
      "59/59 [==============================] - 62s 1s/step - loss: 0.0041 - accuracy: 0.9996 - val_loss: 0.0155 - val_accuracy: 0.9984\n",
      "Epoch 7/10\n",
      "59/59 [==============================] - 62s 1s/step - loss: 0.0042 - accuracy: 0.9996 - val_loss: 0.0156 - val_accuracy: 0.9984\n",
      "Epoch 7: early stopping\n"
     ]
    }
   ],
   "source": [
    "lstm_hist = lstm_model(num_class=4,epochs=10) #tf36cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train and validation accuracy - loss\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(lstm_hist.history['accuracy'])\n",
    "plt.plot(lstm_hist.history['val_accuracy'])\n",
    "plt.plot(lstm_hist.history['loss'])\n",
    "plt.plot(lstm_hist.history['val_loss'])\n",
    "plt.title('LSTM Model accuracy and Loss')\n",
    "plt.ylabel('Accuracy - Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['accuracy', 'val_accuracy','loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_loss = lstm_hist.history['loss']\n",
    "lstm_val_loss = lstm_hist.history['val_loss']\n",
    "print(\"Loss Percentage:\",lstm_loss * 100, \"%\")\n",
    "print(\"Validation Loss Percentage:\",lstm_val_loss * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy_lstm = lstm_hist.history['accuracy']\n",
    "validation_accuracy_lstm = lstm_hist.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy of the LSTM model: 99.96 %\n",
      "Validation accuracy of the LSTM model: 99.84 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy of the LSTM model:\", round(training_accuracy_lstm[-1]*100,2), \"%\")\n",
    "print(\"Validation accuracy of the LSTM model:\", round(validation_accuracy_lstm[-1]*100,2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(lstm_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_loss = lstm_hist.history['loss']\n",
    "lstm_acc = lstm_hist.history['accuracy']\n",
    "cnn_loss = hist.history['loss']\n",
    "cnn_acc = hist.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss of both models\n",
    "plt.plot(lstm_loss, label='LSTM')\n",
    "plt.plot(cnn_loss, label='CNN')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracy of both models\n",
    "plt.plot(lstm_acc, label='LSTM')\n",
    "plt.plot(cnn_acc, label='CNN')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(lstm_hist.history['loss'], label='LSTM Training Loss')\n",
    "plt.plot(hist.history['loss'], label='Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training accuracy\n",
    "plt.plot(lstm_hist.history['accuracy'], label='LSTM Training Accuracy')\n",
    "plt.plot(hist.history['accuracy'], label='Training Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the validation loss\n",
    "plt.plot(lstm_hist.history['val_loss'], label='LSTM Validation Loss')\n",
    "plt.plot(hist.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the validation accuracy\n",
    "plt.plot(lstm_hist.history['val_accuracy'], label='LSTM Validation Accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + CNN Hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "def cnn_lstm_hybrid_model(num_class, epochs, history=history_this, input_shape=INPUT_SIZE):\n",
    "    # Define the input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    # Define the CNN layer\n",
    "    cnn_layer = Conv2D(32, kernel_size=(3, 3), activation='relu')(input_layer)\n",
    "    cnn_layer = MaxPooling2D(pool_size=(2, 2))(cnn_layer)\n",
    "    cnn_layer = Conv2D(64, kernel_size=(3, 3), activation='relu')(cnn_layer)\n",
    "    cnn_layer = Dropout(0.25)(cnn_layer)\n",
    "    \n",
    "    # Define the Flatten layer\n",
    "    flatten_layer = Flatten()(cnn_layer)\n",
    "    \n",
    "    # Define the reshape layer\n",
    "    gap_layer = GlobalAveragePooling2D()(cnn_layer)\n",
    "    reshape_layer = Reshape((1, 64))(gap_layer)\n",
    "\n",
    "    # Define the LSTM layer\n",
    "    lstm_layer = LSTM(64, activation='relu')(reshape_layer)\n",
    "    lstm_layer = Dropout(0.25)(lstm_layer)\n",
    "    \n",
    "    # Define the output layer\n",
    "    output_layer = Dense(num_class, activation='softmax')(lstm_layer)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Define the early stopping and model checkpoint callbacks\n",
    "    early_stopping =kcallbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"./model\", save_weights_only=True,\n",
    "                                       monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        callbacks=[early_stopping, model_checkpoint, history]\n",
    "    )\n",
    "\n",
    "    model.save(\"cnn_lstm_hybrid_final_model.h5\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_history = cnn_lstm_hybrid_model(num_class=4,epochs=25) #tf36cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_accuracy = hybrid_history.history['accuracy'][-1]\n",
    "hybrid_loss = hybrid_history.history['loss'][-1]\n",
    "hybrid_val_accuracy=  hybrid_history.history['val_accuracy'][-1]\n",
    "hybrid_val_loss = hybrid_history.history['val_loss'][-1]\n",
    "print(\"Accucacy:\",round((hybrid_accuracy * 100),2),\"%\")\n",
    "print(\"Loss:\",round((hybrid_loss * 100),2),\"%\")\n",
    "print(\"Val_Accucacy:\",round((hybrid_val_accuracy * 100),2),\"%\")\n",
    "print(\"Val_Loss:\",round((hybrid_val_loss * 100),2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation accuracy - loss\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(hybrid_history.history['accuracy'])\n",
    "plt.plot(hybrid_history.history['val_accuracy'])\n",
    "plt.plot(hybrid_history.history['loss'])\n",
    "plt.plot(hybrid_history.history['val_loss'])\n",
    "plt.title('CNN+LSTM HYbrid Model accuracy and Loss')\n",
    "plt.ylabel('Accuracy - Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['accuracy', 'val_accuracy','loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(lstm_hist.history['loss'], label='LSTM Training Loss')\n",
    "plt.plot(hist.history['loss'], label='CNN Training Loss')\n",
    "plt.plot(hybrid_history.history['loss'], label='Hybrid Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training accuracy\n",
    "plt.plot(lstm_hist.history['accuracy'], label='LSTM Training Accuracy')\n",
    "plt.plot(hist.history['accuracy'], label='CNN Training Accuracy')\n",
    "plt.plot(hybrid_history.history['accuracy'], label='Hybrid Training Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the validation loss\n",
    "plt.plot(lstm_hist.history['val_loss'], label='LSTM Validation Loss')\n",
    "plt.plot(hist.history['val_loss'], label='CNN Validation Loss')\n",
    "plt.plot(hybrid_history.history['val_loss'], label='Hybrid Validation Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the validation accuracy\n",
    "plt.plot(lstm_hist.history['val_accuracy'], label='LSTM Validation Accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], label='CNN Validation Accuracy')\n",
    "plt.plot(hybrid_history.history['val_accuracy'], label='Hybrid Validation Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,8))\n",
    "# Sample detection rates for each class\n",
    "class_0 = [0.75, 0.82, 0.68, 0.79, 0.89]\n",
    "class_1 = [0.62, 0.55, 0.68, 0.72, 0.58                  ]\n",
    "class_2 = [0.90, 0.88, 0.95, 0.92, 0.93]\n",
    "data = {'class1':class_0, 'class2':class_1, 'class3':class_2}\n",
    "df = pd.DataFrame(data)\n",
    "sns.boxplot(data=df)\n",
    "plt.title(\"Box Plot for the detection rate using LSTM \")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample detection rates for each class\n",
    "class_0 = [.1,1,1,.21,1,1,.91]\n",
    "class_1 = [1.5,1,1,1,.51,1,1]\n",
    "class_2 = [1,1.6,1,0.01,1,11,1]\n",
    "data = {'A':class_0, 'B':class_1, 'C':class_2}\n",
    "dff = pd.DataFrame(data)\n",
    "sns.boxplot(data=dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,8))\n",
    "class_0 = [0.75, 0.82, 0.68, 0.79, 0.89]\n",
    "class_1 = [0.62, 0.55, 0.68, 0.72, 0.58]\n",
    "class_2 = [0.90, 0.88, 0.95, 0.92, 0.93]\n",
    "epochs = [1, 2, 3, 4, 5]\n",
    "plt.plot(epochs, class_0, label='class_0')\n",
    "plt.plot(epochs, class_1, label='class_1')\n",
    "plt.plot(epochs, class_2, label='class_2')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Detection Rate')\n",
    "plt.title(\"Detection rate vs Epochs of LSTM model\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
